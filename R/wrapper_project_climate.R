#' Project a time series based on estimated quantile changes
#'
#' \code{build.projection} is a wrapper for \code{\link{project.climate}} that
#' allows for file management, batch processing, bootstrapping, and saving of
#' output, if all the directory and filename/structure defaults are followed,
#' for the projection of climate time series by changes in the distribution
#' shape estimated across a different dataset/model, with potentially multiple
#' runs of data over the same time frame, calculated using
#' \code{\link{wrapper_get_quantiles.R}}.
#'
#' \code{build.projection} runs \code{\link{project.climate}} on a
#' pixel-by-pixel basis. These pixels are loaded and processed on a
#' latitude-by-region/subset basis - in other words, a 'block' of pixels is
#' every pixel in one subset/region (determined by a separate .nc file in the
#' \code{mod.data.dir} set by the \code{\link{set.defaults}} function) with the
#' same latitude. This 'block' of pixels is loaded and sent through
#' \code{\link{get.quantiles}} one at a time; the resultant coefficients are
#' saved in files 'block' by 'block' (and can be recombined later using
#' \code{\link{combine.locs}}). The 'blocks' are identified through the saved
#' output of \code{\link{get.process.chunks}}.
#'
#' CURRENTLY, the code requires that the 'base data' (the data to be projected)
#' is saved in the SAME chunk-by-chunk format as the model data being used to
#' calculated the quantile changes. In other words, each model .nc file in
#' \code{mod.data.dir} must have an equivalent file with the same suffix and
#' covering the same pixels in \code{base.data.dir}. Since memory usage is
#' a much bigger issue in the model quantile fits, a future version of this
#' will allow for the use of more 'standard' base data (not saved in chunks).
#'
#' @section Output: the \code{output="full"} output in
#' \code{\link{project.climate}} option is used, returning (within
#' the function) a list for each latitude-by-region/subset chunk
#' giving the projected time series as an \code{xts} object
#' (\code{proj.data}), the coefficients of the quantiles used for
#' normalizing the base climate time series (\code{base.norm.coef}),
#' the \code{lat} and \code{lon} of the pixel, the \code{output.years},
#' and the linear indices (in the base period) used a the reference
#' year/days in the projected period (\code{base.idxs}). This list
#' object is saved for each processing chunk in the
#' "\code{[defaults$base.data.dir]/output/}" directory, under the
#' filename: "\code{[filevar]_day_[base.name]_[mod.name]proj_[proj.method]_[proj.year.range]_locs[global_loc(1)-global_loc(end)].RData}",
#' where \code{proj.method} is a quick shorthand for the projection
#' method (set by \code{index.type} and \code{resampling.timescale}
#' in \code{defaults}).
#'
#' @section On \code{\link{project.climate}} parameters:
#'  Inputs to \code{\link{project.climate}} are governed through the
#'  \code{defaults} object, generated by the
#'  \code{\link{set.defaults}} function. These include which
#'  data to use as 'base' data for the projection (\code{index.type}
#'  and \code{resampling.timescale}) and bootstrap/uncertainty
#'  quantification parameters (\code{bootstrapping}, \code{nboots},
#'  and \code{block.size}). See \code{\link{project.climate}}
#'  and \code{\link{set.defaults}} for details.
#'
#' @param defaults the output from \code{\link{set.defaults}} (see Section "On
#'   \code{project.climate} parameters," below)
#' @param log whether to log the output using \code{sink()} in the directory
#'   \code{[defaults$aux.data.dir]/run_logs/}. By default, true. Log filenames
#'   are (using run start time):
#'   \code{estimate_quantiles_run_YYYY-MM-DD_HH-MM-SS.txt}.
#' @param max.runtime if batch processing on a server with a maximum allocated
#'   runtime, you can set it here, and the run will stop when the next block of
#'   time might take longer to run than the remaining allocated time. This
#'   prevents empty, un-processed temporary output files from preventing the
#'   complete processing of all data 'blocks'. Explicitly, the next block isn't
#'   processed and the function run is interrupted if the current system time if
#'   \code{start.time + max.runtime - Sys.time() < (2 * 160) * [num pixels in
#'   block] seconds}, assuming that it takes roughly 160 seconds per pixel to
#'   run this code (for 40 runs, 121 years of data, on the server this code was
#'   written on etc.); this time assumption can be changed with the
#'   \code{assumed.avg.processing.time} option, detailed below.
#' @param assumed.avg.processing.time by default 5 (seconds), the estimated
#'   processing time per pixel. Used in interrupting batch run if time is
#'   running out.
#' @param process.inputs add custom process.inputs list. By default, the code
#' 	 attempts to load [defaults$mod.data.dir]/process_inputs.RData. If this is 
#'	 not desired (i.e. you want to just process a subset of process chunks), 
#' 	 put a subsetted output of \code{get.process.chunks}here , or make your own - 
#' 	 just make sure that it's a list of process chunks, with every element 
#' 	 containing at least the fields [global_loc] (used to load the correct
#'	 [params] file), [lat] (just one lat, by lat band), [lon] (all the lon
#'   values desired), and [fn] (the raw data filename from which the params
#'   were calculated).
#'
#' @return Nothing. Output is saved instead.

build.projection <- function(defaults,log=T,
							   max.runtime=4*60*60,assumed.avg.processing.time=5,
							   process.inputs=list()) {

	# ----- SETUP  ----------------------------------------------------------------------
	# Set start time, for logging and cancelling
	start.time <- Sys.time()

	# Write error output to file
	if (log) {
		sink(paste0(defaults$aux.dir,"run_logs/project_climate_run_",gsub('\\:','-',gsub('\\s','_',as.character(start.time))),".txt"))
	}

	# Load processing inputs
	if (length(process.inputs)==0) {
		load(paste0(defaults$mod.data.dir,"process_inputs.RData"))
		# Shuffle process inputs for processing below
		#process.inputs <- process.inputs[sample(seq(1,length(process.inputs)),length(process.inputs))]
	}


	# ----- LOAD BASIS FUNCTIONS  ------------------------------------------------------
	# Load bulk basis function (just one run)
	basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$mod.year.range)+1,"years_",
		"1","runs_",paste0(defaults$bulk.x.df,collapse="-"),"df.RData")
		if (file.exists(basis.fn)) {
	load(basis.fn); bulk.x <- X; rm(list=c("X","basis.fn"))
	} else {
		bulk.x <- get.predictors(n_files=1,dfs=defaults$bulk.x.df,year.range=defaults$mod.year.range,save.predictors=T)
	}

	# Load tail basis function (just one run)
	basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$mod.year.range)+1,"years_",
		"1","runs_",paste0(defaults$tail.x.df,collapse="-"),"df.RData")
	if (file.exists(basis.fn)) {
		load(basis.fn); tail.x <- X; rm(list=c("X","basis.fn"))
	} else {
		tail.x <- get.predictors(n_files=1,dfs=defaults$tail.x.df,year.range=defaults$mod.year.range,save.predictors=T)
	}

	# Load normalization basis function if not using volcanic data
	# (and therefore it doesn't need to be latitude-dependent)
	if (!defaults$get.volc) {
		basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$mod.year.range)+1,"years_",
		"1","runs_",paste0(defaults$norm.x.df,collapse="-"),"df.RData")
		if (file.exists(basis.fn)) {
			load(basis.fn); norm.x <- X; rm(list=c("X","basis.fn"))
		} else {
			norm.x <- get.predictors(n_files=1,dfs=defaults$normx.df,year.range=defaults$mod.year.range,save.predictors=T)
		}
	} else {norm.x <- numeric()}

	# Get process inputs for the base data
	process.inputs.base <- get.process.chunks(defaults,search.dir=defaults$base.data.dir)
	base.lats <- unlist(lapply(1:length(process.inputs.base),function(x) process.inputs.base[[x]]$lat))
	base.regs <- unlist(lapply(1:length(process.inputs.base),function(x) process.inputs.base[[x]]$reg))
	# Match up with model data
	for (chunk in seq(1:length(process.inputs))) {
		process.idx <- which(base.regs == process.inputs[[chunk]]$reg&base.lats == process.inputs[[chunk]]$lat)
		if (length(process.idx)!=0 && identical(process.inputs[[chunk]]$lon,process.inputs.base[[process.idx]]$lon)) {
			process.inputs[[chunk]]$fn_base <- process.inputs.base[[process.idx]]$fn
			process.inputs[[chunk]]$local_idxs_base <- process.inputs.base[[process.idx]]$local_idxs
		} else {
			process.inputs[[chunk]]$fn_base <- process.inputs[[chunk]]$local_idxs_base <- NA
		}
	}
	rm(list=c("base.lats","base.regs","chunk","process.inputs.base"))

	# ----- SET A BY-PROCESS-CHUNK WRAPPER FUNCTION ------------------------------------------------------
	run.map <- function(process.inputs.tmp,
		defaults,
		norm.x=numeric(),bulk.x,tail.x,norm.x.base=numeric(),
		start.time,max.runtime,assumed.avg.processing.time) {

		# See if you even have a match between the model and base data processing chunks
		if (!is.na(process.inputs.tmp$fn_base)) {
			# Get output filename for the quantile fit parameters
			load.fn <- paste0(defaults$mod.data.dir,"params/",defaults$filevar,
										"_day_",defaults$mod.name,"_quantfit_params_",
										defaults$mod.year.range[1],"-",defaults$mod.year.range[2],"_locs",
										process.inputs.tmp$global_loc[1],"-",
										process.inputs.tmp$global_loc[length(process.inputs.tmp$global_loc)])
			if (defaults$bootstrapping) {
				load.fn <- paste0(load.fn,"_",defaults$block.size,"block",defaults$nboots,"runs")
			}
			load.fn <- paste0(load.fn,".RData")

			# See if that filename exists/if those parameters have been calculated...
			if (file.exists(load.fn)) {

				# Get better string filename summary for the projection base year choosing
				if (defaults$index.type=="resampling_rep") {
					proj.method <- "resample"
				} else if (defaults$index.type=="resampling") {
					proj.method <- "NoRepResample"
				} else if (defaults$index.type=="raw") {
					proj.method <- "projectby"
				}
				proj.method<-paste0(proj.method,defaults$resampling.timescale)

				# Set saving filename
				output.fn <- paste0(defaults$base.data.dir,"output/",defaults$filevar,"_day_",
										defaults$base.name,"_",defaults$mod.name,"proj_",proj.method,"_",
										paste0(defaults$proj.year.range,collapse="-"),"_",
										 "locs",process.inputs.tmp$global_loc[1],"-",
										 process.inputs.tmp$global_loc[length(process.inputs.tmp$global_loc)])

				if (defaults$bootstrapping) {
					output.fn <- paste0(output.fn,"_",defaults$block.size,"block",defaults$nboots,"runs")
				}

				output.fn <- paste0(output.fn,".RData")
				rm(proj.method)


				# Only process if the file doesn't already exist
				if (!file.exists(output.fn)) {

					# Only process if there's enough time left - this is determined by
					# seeing if the time elapsed since the start of the run is more than
					# 2 x the average processing time expected for the number of pixels
					# in this run (assuming ~160 seconds / pixel)
					if ((~defaults$bootstrapping && difftime(Sys.time(),start.time,units="secs") < (max.runtime)-2*assumed.avg.processing.time*length(process.inputs.tmp$global_loc)) ||
						(difftime(Sys.time(),start.time,units="secs") < (max.runtime)-2*assumed.avg.processing.time*length(process.inputs.tmp$global_loc*defaults$nboots))) {

						# Save a placeholder file in the directory; to make sure no
						# double-processing happens
						status <- 'processing'
						save(file=output.fn,
							status)
						rm(status)

						# Input into a tryCatch statement to allow deletion of the temporary
						# file in the case of an error (which hopefully will never happen,
						# but yaknow...)
						tryCatch({

							# ----- SETUP ----------------------------------------------------
							cat("\n")
							cat(paste0("Beginning processing for pixels with lat=",process.inputs.tmp$lat,
								" in region ",process.inputs.tmp$reg," (global locations ",process.inputs.tmp$global_loc[1],"-",
								process.inputs.tmp$global_loc[length(process.inputs.tmp$global_loc)],")"),fill=TRUE)

							# Get file year range (from position in filename, FILENAME MUST
							# BE IN CMIP5 STANDARD BY YEAR, but can be either YYYYMMDD or YYYY)
							fn.year.range <- strtoi(substr(strsplit(strsplit(process.inputs.tmp$fn_base,'\\_|\\.')[[1]][6],'\\-')[[1]],1,4))

							# Load LENS fit parameters
							load(load.fn)
							rm(load.fn)

							# ----- LOAD LAT-DEPENDENT BASES ---------------------------------
							# Load reanalyisis normalizing basis functions
							if (length(norm.x.base)==0) {
								if (defaults$get.volc) {
									# Get lats from the volcanic data to pick right basis file
									ncdata.volc <- nc_open(defaults$volc.path)
								    lat.volc <- ncvar_get(ncdata.volc, 'lat')
								    nc_close(ncdata.volc)
								    # Load basis file
								    basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$base.year.range)+1,"years_1runs_",
								    	paste0(defaults$base.norm.x.df,collapse="-"),
								    	"df_volc",gsub("\\.","-",round(lat.volc[which.min(abs(as.vector(lat.volc)-as.vector(process.inputs.tmp$lat)))],1)),".RData")
								    if (file.exists(basis.fn)) {
								    	load(basis.fn);norm.x.base <- X; rm(X)
								    } else {
								    	norm.x.base <- get.predictors(n_files=1,dfs=defaults$base.norm.x.df,year.range=defaults$base.year.range,get.volc=TRUE,lat=process.inputs.tmp$lat)
								    }
								    rm(list=c("basis.fn","ncdata.volc","lat.volc"))
								} else {
									basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$base.year.range)+1,"years_1runs_",paste0(defaults$base.norm.x.df,collapse="-"),"df.RData")
									if (file.exists(basis.fn)) {load(basis.fn);norm.x.base <- X; rm(X)} else {norm.x.base <- get.predictors(n_files=1,dfs=defaults$base.norm.x.df,year.range=defaults$base.year.range,save.predictors=T)}
								}
							}

						    # Get spline basis functions for normalization fit
						    if (length(norm.x)==0) {
						        if (defaults$get.volc) {
						            ncdata.volc <- nc_open(defaults$volc.path);lat.volc <- ncvar_get(ncdata.volc, 'lat');nc_close(ncdata.volc)
						            # Load basis file
						            basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$base.year.range)+1,"years_1runs_",
						                paste0(defaults$base.norm.x.df,collapse="-"),
						                "df_volc",gsub("\\.","-",round(lat.volc[which.min(abs(as.vector(lat.volc)-as.vector(process.inputs.tmp$lat)))],1)),".RData")
						            if (file.exists(basis.fn)) {load(basis.fn);norm.x<-X;rm(x)} else {norm.x <- get.predictors(n_files=1,dfs=params[[1]]$norm.x.df,year.range=params[[1]]$year.range,get.volc=TRUE,lat=process.inputs.tmp$lat,save.predictors=T)}
						            rm(list=c("basis.fn","ncdata.volc","lat.volc"))
						        } else {
						            basis.fn <- paste0(defaults$aux.dir,"bases/spline_basis_functions_",diff(defaults$base.year.range)+1,"years_1runs_",paste0(params$norm.x.df,collapse="-"),"df.RData")
						            if (file.exists(basis.fn)) {load(basis.fn);norm.x<-X;rm(x)} else {norm.x <- get.predictors(n_files=1,dfs=params[[1]]$norm.x.df,year.range=-params[[1]]$year.range,save.predictors=T)}
						        }
						    }

						    # ----- LOAD AND SET UP RAW BASE DATA ---------------------------------------------
							# Load raw reanalyis netcdf data, by latitude band within a region
						    # This gets the raw data form the netcdf, permutes it so the time dimension is first,
							# then cbinds along the "runs" dimension to get a single time series for each point
							# (a [nyear*365*nruns x npoints] array)
							Raw <- get.ncdf(defaults,process.inputs.tmp,year.range=defaults$base.year.range)

							# Raw needs to be an xts object for stupid reasons
							t.out <- do.call("c",lapply(defaults$base.year.range[1]:defaults$base.year.range[2],
								function(yr) {timeBasedSeq(paste0(yr,'/',yr,'/d'))}))
				     		t.out <- t.out[strftime(t.out,format="%j")!='366'] #(removing leap years)
				     		Raw <- lapply(Raw,function(R) R$Raw <- xts(R$Raw,order.by=t.out))

							# Repeat time series out for each element
							if (defaults$bootstrapping) {
								# Assign to avoid the new variable getting lost in a weird cache possibly created
								# by the if statement above. Running "Raw <- " results in incorrect assigning of
								# params in the for loop below, for reasons not yet solved by me or the internet.
								assign('Raw',rep(Raw,each=defaults$nboots))

								# Re-assign params so that each copy is assigned a different bootstrapped param fit file
								for (x in seq(1,length(params))) {
									Raw[[x]]$params <- params[[x]]
								}

								# Correspondingly change in the process.inputs.tmp list to make the
								# parameter information assigning below easier
								process.inputs.tmp$global_loc <- rep(process.inputs.tmp$global_loc,each=defaults$nboots)
								process.inputs.tmp$run.idx <- rep(1:defaults$nboots,length(process.inputs.tmp$lon))
							}


							# ----- PROJECT DATA USING PROJECT.CLIMATE ---------------------------------
							output.map <- lapply(Raw,function(input.list) {
								cat("\n") #Insert newlines to add space between messages of different processing chunks
								if (defaults$bootstrapping) {
									cat(paste0("processing lon = ",input.list$lon,", run ",input.list$params$run.idx),fill=TRUE)
								} else {
									cat(paste0("processing lon = ",input.list$lon),fill=TRUE)
								}

								project.climate(defaults,input.list$params,base.data=input.list$Raw,
									output.years=seq(defaults$proj.year.range[1],defaults$proj.year.range[2]),
									rand.seed.set=42,
									norm.x.base=norm.x.base,
									norm.x=norm.x,
									bulk.x=bulk.x,
									tail.x=tail.x,
									output="full",index.type=defaults$index.type,
									resampling.timescale=defaults$resampling.timescale)
								})

							# ----- CLEAN UP AND SAVE -------------------------------------------------
							# Add a few extra identifiers to outputted list
							for (loc.idx in 1:length(params)) {
								output.map[[loc.idx]]$global_loc <- process.inputs.tmp$global_loc[loc.idx]
								output.map[[loc.idx]]$reg <- process.inputs.tmp$reg
								if (defaults$get.volc) {
									output.map[[loc.idx]]$inc.volc <- TRUE
								} else {
									output.map[[loc.idx]]$inc.volc <- FALSE
								}
								if (defaults$bootstrapping) {
									output.map[[loc.idx]]$run.idx <- process.inputs.tmp$run.idx[loc.idx]
								}
							}

							# Save output
							save(file=output.fn,output.map)
							cat(paste0(defaults$base.name," projection complete and saved for pixels with lat=",process.inputs.tmp$lat,
								" in region ",process.inputs.tmp$reg," (global locations ",process.inputs.tmp$global_loc[1],"-",
								process.inputs.tmp$global_loc[length(process.inputs.tmp$global_loc)],")!"),fill=TRUE)
							cat("\n")

						},error=function(e) {
							print(e)
							# Remove the temporary file if the run has an error
							file.remove(output.fn)
							cat(paste0("Error occured on the ",defaults$base.name," projection run for pixels with lat=",process.inputs.tmp$lat,
								" in region ",process.inputs.tmp$reg," (global locations ",process.inputs.tmp$global_loc[1],"-",
								process.inputs.tmp$global_loc[length(process.inputs.tmp$global_loc)],")!"),fill=TRUE)
							cat("\n")
						})

					} else {
						stop(paste0("Run stopped at ",Sys.time()," after ",format(difftime(Sys.time(),start.time)),
							" due to set server time limits."))
					}
				} else {
					cat(paste0(output.fn," already exists; skipping.\n"),fill=TRUE)
				}
			} else {
				cat(paste0("No quantile fit parameters found for global locations ",process.inputs.tmp$global_loc[1],"-",
						process.inputs.tmp$global_loc[length(process.inputs.tmp$global_loc)]," (searched for file: ",load.fn,"); skipping.\n"),fill=TRUE)
			}
		} else {
			cat(paste0("The pixels for the quantile fit parameters in ",process.inputs.tmp$fn," could not be matched with a base data process chunk; skipping.\n"),fill=TRUE)
		}

		# Return nothing
		invisible()
	}


	#----- RUN ------------------------------------------------------
	lapply(process.inputs,run.map,
		defaults=defaults,norm.x=norm.x,bulk.x=bulk.x,tail.x=tail.x,
		start.time=start.time,max.runtime=max.runtime,
		assumed.avg.processing.time=assumed.avg.processing.time)

	#----- SOME HOUSEKEEPING ----------------------------------------
	# CLose out log
	if (log) {sink()}

	# Don't return anything...
	invisible()
}
